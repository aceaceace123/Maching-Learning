{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This job will be divided into two parts.\n",
    "- First, introduce some classification theories.\n",
    "- Second, implement with laboratory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>Introduce of Support vector machine</font>\n",
    "Sometimes we faced with non-linear class boundaries,so we can't use support vector classifier.Then we exten support vector classifier to support vector machine(SVM).This is a technique of enlarging the feature space in a specific way, using kernels.We want to use SVM to find the hyperplane that can classify different labels (y).The hyperplane can be writen :$$H:W^T \\phi(x)+b=0$$ where,w is that matrix representing the plane's parameters also the coefficient of x where x is the input data. b represents the intercept of the hyperplane.And we should optime to find w,  $W^*=\\mathop{argmax}\\limits_{W}\\frac{1}{\\|w\\|}[\\mathop{min}\\limits_{n}y_{n}[W^T \\phi(x)+b] ]$.\n",
    "\n",
    "$\\phi(\\cdot)$ is a kernels function,it can enlarge original data space to higher dimenesion.Some common kernels :\n",
    "- Linear kernel:$\\phi(x,x^*)=x\\cdot x'$.It usually solve binary category,because this kernel transform orginal data to 2-dimension.But it present poorly if we have more than two categorys.\n",
    "- Polynomial kernel:$\\phi(x,x^*)=(1+\\sum_{j=1}^{p}x_{ij}x^*_{ij})^d$,using the parameter 'd' can enlarge any dimenesion which depeond on our category.This kernel suit for some of condition that we can't classify on low dimenesion.In the same time,the parameter 'd' could determine by <font color=red>cross validation.</font>\n",
    "- Radial Basis Function:$\\phi(x,x^*)=\\exp(-\\gamma \\sum_{j=1}^{p}(x_{ij}-x^{*}_{ij})^2),\\gamma$ is a posite constant.The RBF kernel maps samples to an infinite-dimensional space, making it possible to find a linear boundary that separates the data with high accuracy in this space. The kernel trick is used to compute the inner products in the higher-dimensional space without actually computing the coordinates of the samples in that space.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>Introduce of some neural network alogrithms</font>\n",
    "The MLP algorithm is particularly useful for classification tasks, where the goal is to predict a discrete label based on a set of input features. The MLP consists of multiple layers of neurons, including an input layer, one or more hidden layers, and an output layer. Each neuron in the MLP is connected to neurons in the adjacent layers, and the weights of these connections are learned during the training process.There are some famous alogrithm in MLPClassifier.\n",
    "- LBFGS（Limited-memory Broyden-Fletcher-Goldfarb-Shanno）:$$x_{k+1}=x_{k}-\\alpha_{k}H_{k}g_{k} $$\n",
    "where $\\alpha_k$ is the step length, $g_k$ is the gradient, and $H_k$ is the inverse estimate of the Hessian matrix calculated from the differences between past gradients and positions.\n",
    "The basic idea of LBFGS is to restrict the storage of past gradients and position differences in BFGS, thereby avoiding the need to store large matrices.<font color=blue> This solver is a good choice for small datasets, but may have difficulty scaling to larger datasets.</font>\n",
    "- Stochastic Gradient Descent(SGD):$$w_{t+1} = w_t - \\eta \\nabla Q_i(w_t, x_i, y_i)$$\n",
    " where $w_t$ represents the weight at the $t$-th iteration, $\\eta$ is the learning rate, and $\\nabla Q_i(w_t, x_i, y_i)$ is the gradient of the loss function, usually calculated based on a single sample $i$.SGD updates the parameters of the model in each iteration by taking small steps in the direction of the negative gradient of the objective function evaluated on a single sample or a mini-batch of samples from the training data.<font color=blue> This solver is often used for large datasets, but may require more hyperparameter tuning than other solvers.</font>\n",
    " - Adaptive Moment Estimation(ADAM):$$w_{t+1}=w_{t}-\\frac{\\eta}{\\sqrt{\\hat{v}_t}+\\epsilon}\\hat{m}_t$$\n",
    " where $w_t$ represents the weights in the $t^{th}$ iteration, $\\eta$ represents the learning rate, $\\hat{m}_t$ and $\\hat{v}_t$ are the bias-corrected estimates of the first and second moments of the gradients, respectively, and $\\epsilon$ is a small number, typically set to $10^{-8}$.The Adam algorithm is based on the first-order moment estimate of gradients, also known as momentum, and the second-order moment estimate of gradients, which is the exponentially weighted average of the square of gradients. Specifically, it maintains a momentum vector $m_t$ and a exponentially weighted average vector of squared gradients $v_t$, and uses them to update the weights. <font color=blue> It has been shown to work well on a wide range of problems, and is often the default choice for MLPClassifier.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>Introduce alogrithms in LogisticRegression</font>\n",
    "- LBFGS:we had introduced before.\n",
    "- \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laboratory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC,LinearSVC,LinearSVR\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/Users/liang/Documents/淺度機器學習/資料集/Wine.xlsx')\n",
    "X = np.array(df.iloc[:, :-1]) # 排除最後一欄標籤 \n",
    "y = np.array(df.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)#每次執行的結果會不一樣"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_ = scaler.fit_transform(X_train)\n",
    "X_test_ = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use orginal data to train the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.15%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.95      0.97        19\n",
      "           2       0.95      1.00      0.98        20\n",
      "           3       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n",
      "98.15%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.95      0.97        19\n",
      "           2       0.95      1.00      0.98        20\n",
      "           3       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n",
      "100.00%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        19\n",
      "           2       1.00      1.00      1.00        20\n",
      "           3       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           1.00        54\n",
      "   macro avg       1.00      1.00      1.00        54\n",
      "weighted avg       1.00      1.00      1.00        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opts = dict(tol = 1e-6, max_iter = int(1e6), verbose=0) #1e6=10^6,verbose=1表示想看到過程\n",
    "# solver = 'lbfgs' # ’lbfgs’ is the default# solver = ’liblinear’# solver = ’newton−cg’\n",
    "clf_original_lbfgs= LogisticRegression(solver = 'lbfgs', **opts)\n",
    "clf_original_newtoncg= LogisticRegression(solver = 'newton-cg', **opts)\n",
    "clf_original_liblin= LogisticRegression(solver = 'liblinear', **opts)\n",
    "clf_original_lbfgs.fit(X_train_, y_train)\n",
    "y_pred = clf_original_lbfgs.predict(X_test_)\n",
    "# print(f'{accuracy_score(y_test, y_pred):.2%}\\n') #return the fraction of correctly classified samples \n",
    "print(f'{clf_original_lbfgs.score(X_test_, y_test):.2%}\\n') #Return the mean accuracy on the given test data and labels\n",
    "print(classification_report(y_test, y_pred))\n",
    "clf_original_newtoncg.fit(X_train_, y_train)\n",
    "y_preds = clf_original_newtoncg.predict(X_test_) \n",
    "print(f'{clf_original_newtoncg.score(X_test_, y_test):.2%}\\n') #Return the mean accuracy on the given test data and labels\n",
    "print(classification_report(y_test, y_preds))\n",
    "clf_original_liblin.fit(X_train_, y_train)\n",
    "y_predss = clf_original_liblin.predict(X_test_)\n",
    "print(f'{clf_original_liblin.score(X_test_, y_test):.2%}\\n') #Return the mean accuracy on the given test data and labels\n",
    "print(classification_report(y_test, y_predss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use orginal data to train the support vector machine model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        19\n",
      "           2       0.95      1.00      0.98        20\n",
      "           3       1.00      0.93      0.97        15\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        19\n",
      "           2       0.95      1.00      0.98        20\n",
      "           3       1.00      0.93      0.97        15\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.84      0.89        19\n",
      "           2       0.83      0.95      0.88        20\n",
      "           3       1.00      0.93      0.97        15\n",
      "\n",
      "    accuracy                           0.91        54\n",
      "   macro avg       0.92      0.91      0.91        54\n",
      "weighted avg       0.91      0.91      0.91        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C=1\n",
    "opts=dict(C=C,tol=1e-6,max_iter=int(1e6))\n",
    "# opts=dict(C=C,decision_function_shape='ovo',tol=1e-6,max_iter=int(1e6))\n",
    "clf_svm_linear=SVC(kernel='linear',**opts)\n",
    "clf_svm_rbf=SVC(kernel='rbf',gamma=0.2,**opts)\n",
    "clf_svm_poly=SVC(kernel='poly',degree=3,**opts)\n",
    "# clf_svm=LinearSVC(**opts) #one vs the rest\n",
    "clf_svm_linear.fit(X_train_,y_train)\n",
    "predictions=clf_svm_linear.predict(X_test_)\n",
    "print(classification_report(y_test,predictions))\n",
    "clf_svm_rbf.fit(X_train_,y_train)\n",
    "predictions=clf_svm_rbf.predict(X_test_)\n",
    "print(classification_report(y_test,predictions))\n",
    "clf_svm_poly.fit(X_train_,y_train)\n",
    "predictions=clf_svm_poly.predict(X_test_)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sol\\>precision = 真正例 / (真正例 + 假正例)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54,)\n",
      "[2 2 2 1 3 2 3 3 2 2 1 2 2 2 1 3 1 2 1 3 3 1 2 1 3 1 2 2 2 3 1 2 2 2 1 2 1\n",
      " 1 3 1 2 3 1 2 1 1 3 2 3 2 3 2 3 1]\n",
      "[1 2 2 1 3 2 3 3 2 2 1 2 2 2 1 3 1 2 1 3 3 1 2 1 3 1 2 2 2 3 1 2 3 2 1 2 1\n",
      " 1 3 1 1 3 1 2 1 2 3 2 3 2 3 1 3 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)\n",
    "print(predictions)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use orginal data to train the ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for test data:98.15%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.95      0.97        19\n",
      "           2       0.95      1.00      0.98        20\n",
      "           3       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n",
      "accuracy for test data:96.30%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.95      0.97        19\n",
      "           2       0.91      1.00      0.95        20\n",
      "           3       1.00      0.93      0.97        15\n",
      "\n",
      "    accuracy                           0.96        54\n",
      "   macro avg       0.97      0.96      0.96        54\n",
      "weighted avg       0.97      0.96      0.96        54\n",
      "\n",
      "accuracy for test data:98.15%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.95      0.97        19\n",
      "           2       0.95      1.00      0.98        20\n",
      "           3       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hidden_layers = (512,) # one hidden layer \n",
    "# # activation = ’relu’ # the default \n",
    "hidden_layers = (30,)\n",
    "activation = 'logistic'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers, verbose = False, \\\n",
    "        activation = activation, tol = 1e-6, max_iter = int(1e6))\n",
    "# solver = ’sgd’ # not efficient, need more tuning \n",
    "# solver = ’lbfgs’ # not suitable here #適用連續資料\n",
    "# solver = 'adam' # default solver #適用類別資料\n",
    "clf_MLP_adam = MLPClassifier(solver = 'adam', **opts) \n",
    "clf_MLP_lbfgs = MLPClassifier(solver = 'lbfgs', **opts) \n",
    "clf_MLP_sgd = MLPClassifier(solver = 'sgd', **opts) \n",
    "clf_MLP_adam.fit(X_train_, y_train)\n",
    "predictions = clf_MLP_adam.predict(X_test_)\n",
    "print('accuracy for test data:{:.2f}%'.format(100*clf_MLP_adam.score(X_test_,y_test)))\n",
    "print(classification_report(y_test, predictions))\n",
    "clf_MLP_lbfgs.fit(X_train_, y_train)\n",
    "predictions = clf_MLP_lbfgs.predict(X_test_)\n",
    "print('accuracy for test data:{:.2f}%'.format(100*clf_MLP_lbfgs.score(X_test_,y_test)))\n",
    "print(classification_report(y_test, predictions))\n",
    "clf_MLP_sgd.fit(X_train_, y_train)\n",
    "predictions = clf_MLP_sgd.predict(X_test_)\n",
    "print('accuracy for test data:{:.2f}%'.format(100*clf_MLP_sgd.score(X_test_,y_test)))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use principal component analysis DATA to train LOGISTIC REGRESSION model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.30%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.89      0.94        19\n",
      "           2       0.91      1.00      0.95        20\n",
      "           3       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           0.96        54\n",
      "   macro avg       0.97      0.96      0.97        54\n",
      "weighted avg       0.97      0.96      0.96        54\n",
      "\n",
      "96.30%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.89      0.94        19\n",
      "           2       0.91      1.00      0.95        20\n",
      "           3       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           0.96        54\n",
      "   macro avg       0.97      0.96      0.97        54\n",
      "weighted avg       0.97      0.96      0.96        54\n",
      "\n",
      "96.30%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.89      0.94        19\n",
      "           2       0.91      1.00      0.95        20\n",
      "           3       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           0.96        54\n",
      "   macro avg       0.97      0.96      0.97        54\n",
      "weighted avg       0.97      0.96      0.96        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components = 0.8).fit(X_train_)#n_components = 0.8取80%的能量，或著可以寫成n_components = 10取前十個主成分\n",
    "Z_train = pca.transform(X_train_)\n",
    "Z_test = pca.transform(X_test_)\n",
    "opts = dict(tol = 1e-6, max_iter = int(1e6), verbose=0) \n",
    "# solver = 'lbfgs' # ’lbfgs’ is the default# solver = ’liblinear’# solver = ’newton−cg’\n",
    "clf_PCA_lbfgs = LogisticRegression(solver = 'lbfgs', **opts)\n",
    "clf_PCA_liblin = LogisticRegression(solver = 'liblinear', **opts)\n",
    "clf_PCA_newtoncg = LogisticRegression(solver = 'newton-cg', **opts)\n",
    "clf_PCA_lbfgs.fit(Z_train, y_train)\n",
    "y_pred = clf_PCA_lbfgs.predict(Z_test)\n",
    "# print(f'{accuracy_score(y_test, y_pred):.2%}\\n')\n",
    "print(f'{clf_PCA_lbfgs.score(Z_test, y_test):.2%}\\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "clf_PCA_liblin.fit(Z_train, y_train)\n",
    "y_pred = clf_PCA_liblin.predict(Z_test)\n",
    "print(f'{clf_PCA_liblin.score(Z_test, y_test):.2%}\\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "clf_PCA_newtoncg.fit(Z_train, y_train)\n",
    "y_pred = clf_PCA_newtoncg.predict(Z_test)\n",
    "print(f'{clf_PCA_newtoncg.score(Z_test, y_test):.2%}\\n')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use PCA data to train the support vector machine model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.89      0.94        19\n",
      "           2       0.91      1.00      0.95        20\n",
      "           3       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           0.96        54\n",
      "   macro avg       0.97      0.96      0.97        54\n",
      "weighted avg       0.97      0.96      0.96        54\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        19\n",
      "           2       0.95      1.00      0.98        20\n",
      "           3       1.00      0.93      0.97        15\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.79      0.86        19\n",
      "           2       0.79      0.95      0.86        20\n",
      "           3       1.00      0.93      0.97        15\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.91      0.89      0.90        54\n",
      "weighted avg       0.90      0.89      0.89        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C=1\n",
    "opts=dict(C=C,tol=1e-6,max_iter=int(1e6))\n",
    "# opts=dict(C=C,decision_function_shape='ovo',tol=1e-6,max_iter=int(1e6))\n",
    "clf_svm_pca_linear=SVC(kernel='linear',**opts)\n",
    "clf_svm_pca_rbf=SVC(kernel='rbf',gamma=0.2,**opts)\n",
    "clf_svm_pca_poly=SVC(kernel='poly',degree=3,**opts)\n",
    "# clf_svm=LinearSVC(**opts) #one vs the rest\n",
    "clf_svm_pca_linear.fit(Z_train,y_train)\n",
    "predictions=clf_svm_pca_linear.predict(Z_test)\n",
    "print(classification_report(y_test,predictions))\n",
    "clf_svm_pca_rbf.fit(Z_train,y_train)\n",
    "predictions=clf_svm_pca_rbf.predict(Z_test)\n",
    "print(classification_report(y_test,predictions))\n",
    "clf_svm_pca_poly.fit(Z_train,y_train)\n",
    "predictions=clf_svm_pca_poly.predict(Z_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use PCA data to train the ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.95      0.97        19\n",
      "           2       0.95      1.00      0.98        20\n",
      "           3       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.95      0.97        19\n",
      "           2       0.95      1.00      0.98        20\n",
      "           3       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.89      0.94        19\n",
      "           2       0.87      1.00      0.93        20\n",
      "           3       1.00      0.93      0.97        15\n",
      "\n",
      "    accuracy                           0.94        54\n",
      "   macro avg       0.96      0.94      0.95        54\n",
      "weighted avg       0.95      0.94      0.95        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hidden_layers = (512,) # one hidden layer \n",
    "# # activation = ’relu’ # the default \n",
    "hidden_layers = (30,)\n",
    "activation = 'logistic'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers, verbose = False, \\\n",
    "        activation = activation, tol = 1e-6, max_iter = int(1e6))\n",
    "# solver = ’sgd’ # not efficient, need more tuning \n",
    "# solver = ’lbfgs’ # not suitable here #適用連續資料\n",
    "# solver = 'adam' # default solver #適用類別資料\n",
    "clf_MLP_pca_adam = MLPClassifier(solver = 'adam', **opts) \n",
    "clf_MLP_pca_lbfgs = MLPClassifier(solver = 'lbfgs', **opts) \n",
    "clf_MLP_pca_sgd = MLPClassifier(solver = 'sgd', **opts) \n",
    "clf_MLP_pca_adam.fit(Z_train, y_train)\n",
    "predictions = clf_MLP_pca_adam.predict(Z_test)\n",
    "# print('accuracy for test data:{:.2f}%'.format(100*clf_MLP.score(Z_test,y_test)))\n",
    "print(classification_report(y_test, predictions))\n",
    "clf_MLP_pca_lbfgs.fit(Z_train, y_train)\n",
    "predictions = clf_MLP_pca_lbfgs.predict(Z_test)\n",
    "# print('accuracy for test data:{:.2f}%'.format(100*clf_MLP.score(Z_test,y_test)))\n",
    "print(classification_report(y_test, predictions))\n",
    "clf_MLP_pca_sgd.fit(Z_train, y_train)\n",
    "predictions = clf_MLP_pca_sgd.predict(Z_test)\n",
    "# print('accuracy for test data:{:.2f}%'.format(100*clf_MLP.score(Z_test,y_test)))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
